#################
# GLOBAL PARAMS #
#################
seed: &seed 42


###############
# Data params #
###############
dataset_params:
  raw_dataset_file: "/home/ppradhan/Documents/my_learnings/my_uni_stuffs/sa_data_storage/training.1600000.processed.noemoticon.csv"
  max_length: 128
  batch_size: 32




###################
# Training params #
###################
training_params:
  base-model-name: "cardiffnlp/twitter-roberta-base"
  num_classes: 2
  train_mode: "fine_tune_base_model"
  tokenizer: 'bert-base-uncased'
  learning_rate: 1e-5
  num_epochs: 5

  optimizer:
    type_: adamW
    lr: 1.e-5
    weight_decay: 0.01

  lr_scheduler:
    type_: 'lin_warmup'
    interval: 'step'
    lr_warmup: 0.1

  trainer:
    max_steps: 2.e+5
#    max_epochs: 20
#    accumulate_grad_batches: 1 # use when memory is not sufficient
    gradient_clip_val: 1
    precision: bf16-mixed
#    precision: 16-mixed
    log_every_n_steps: 50
    val_check_interval: 1000
  logging:
    log_dir: "/home/ppradhan/Documents/my_learnings/my_uni_stuffs/logs"

  callbacks:
    dirpath: "/home/ppradhan/Documents/my_learnings/my_uni_stuffs/logs/models"
    monitor_var: 'train_loss_step'
    monitor_var_mode: 'max'
    save_top_k: 1
