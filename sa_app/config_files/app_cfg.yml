#################
# GLOBAL PARAMS #
#################
seed: &seed 42


###############
# Data params #
###############
dataset_params:
  wandb_storage: # If this is defined the data artifacts are downloaded from wandb
    wandb_user_id: "prabhupad26"
    wandb_project_name: "sa-roberta"
    wandb_artifact_name: "sentiment-dataset"
    wandb_artifact_type: "training_dataset"
    wandb_artifact_version: "v1" # mention the specific version here
    training_file_type: "csv"
    labels_mapping_file_name: "mapping.txt"
  local_storage: # If wandb_storage is not defined the data artifacts referred from local path configured below
    raw_dataset_file: "/home/ppradhan/Documents/my_learnings/my_uni_stuffs/sa_data_storage/training.1600000.processed.noemoticon.csv"
    labels_mapping: "/home/ppradhan/Documents/my_learnings/my_uni_stuffs/bdp2_apr22_exam-bdp2_apr22_group_2/sa_app/config_files/mapping.txt"
  max_length: 128
  batch_size: 32
  preprocessors:
    base_cleaning:
      {}
    lowcase:
      {}
    stem:
      language: 'english'
    lemma:
      model_name: 'en_core_web_sm'




###################
# Training params #
###################
training_params:
  base-model-name: "cardiffnlp/twitter-roberta-base"
  num_classes: 2
  train_mode: "fine_tune_base_model"
  tokenizer: 'bert-base-uncased'
  learning_rate: 1e-5
  num_epochs: 5

  optimizer:
    type_: adamW
    lr: 1.e-5
    weight_decay: 0.01

  lr_scheduler:
    type_: 'lin_warmup'
    interval: 'step'
    lr_warmup: 0.1

  trainer:
    max_steps: 10
#    max_epochs: 20
#    accumulate_grad_batches: 1 # use when memory is not sufficient
    gradient_clip_val: 1
    precision: bf16-mixed
#    precision: 16-mixed
    log_every_n_steps: 50
    val_check_interval: 5
    limit_val_batches: 2

  wandb_storage:
    artifact_name: "best_model_checkpoint"
    artifact_type: "model"
    register_to: "model-registry/sentiment-analysis-classifier"

  logging:
    log_dir: "/home/ppradhan/Documents/my_learnings/my_uni_stuffs/logs"

  callbacks:
    dirpath: "/home/ppradhan/Documents/my_learnings/my_uni_stuffs/logs/models"
    monitor_var: 'valid_acc_step'
    monitor_var_mode: 'max'
    save_top_k: 1


####################
# Inference params #
####################

inference_params:
  model_dir: "cardiffnlp/twitter-roberta-base"
